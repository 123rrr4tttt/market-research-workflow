# 文档去重逻辑说明

> 最后更新：2026-02 | 文档索引：`docs/README.md`

## 问题
用户询问：搜索结果中的文档被跳过是因为真的重复了吗？

## 答案
**不是"重复"的问题，而是"URI已存在"的问题。**

## 去重逻辑详解

### 1. 去重检查顺序

在 `app/services/discovery/store.py` 的 `store_results()` 函数中，去重检查按以下顺序进行：

#### 第一步：URI检查（主要去重方式）
```python
doc = session.query(Document).filter(Document.uri == link).one_or_none()
if doc:
    # 文档已存在，进行轻量更新
    ...
```

**逻辑**：
- 如果URI已存在于数据库中，认为文档已存在
- 这不是"内容重复"，而是"URL重复"
- 同一个URL可能在不同时间被搜索到，这是正常的

#### 第二步：text_hash检查（备用去重方式）
```python
text_hash = _sha256((content or title) + "\n" + link)
exists = session.query(Document).filter(Document.text_hash == text_hash).one_or_none()
if exists:
    skipped += 1
    continue
```

**逻辑**：
- 如果URI不存在，但text_hash相同，也认为是重复
- text_hash基于内容+URL计算，用于检测内容完全相同的文档

### 2. 已存在文档的处理

当发现URI已存在时，代码会进行**轻量更新**（light update）：

```python
if doc:
    # 1. 如果缺少publish_date，重新抓取尝试提取
    if not doc.publish_date:
        content_for_date, soup_for_date, headers_for_date = _fetch_content(link)
        publish_date = _extract_publish_date(...)
        if publish_date:
            doc.publish_date = publish_date
            changed = True
    
    # 2. 补充其他缺失字段
    if not doc.title and title:
        doc.title = title
        changed = True
    # ... 其他字段
    
    # 3. 如果有任何更新，标记为updated；否则标记为skipped
    if changed:
        updated += 1
    else:
        skipped += 1
```

### 3. 本次搜索结果分析

#### 搜索结果统计
- **找到结果**: 10个
- **存储统计**: `inserted: 0, updated: 0, skipped: 10`

#### 跳过原因分析

所有10个文档都被跳过，原因如下：

1. **URI已存在** ✅
   - 所有10个文档的URI都已存在于数据库中
   - 这是正确的去重行为，避免重复插入

2. **更新情况**
   - 大部分文档已有完整的字段（title, summary, publish_date等）
   - 部分文档缺少publish_date，但日期提取失败，无法更新
   - 因为没有字段需要更新，所以标记为`skipped`

#### 具体案例

| 文档ID | 标题 | 发布日期 | 跳过原因 |
|--------|------|---------|---------|
| 138 | US Lottery Market Growth Analysis | ✅ 2025-11-07 | URI已存在，已有发布日期 |
| 127 | Lottery Market in the US to Grow | ✅ 2025-02-12 | URI已存在，已有发布日期 |
| 175 | Online Lottery Market Size & Share | ❌ 缺失 | URI已存在，日期提取失败 |
| 119 | Lottery Market Size, Share & Trends | ✅ 2024-01-01 | URI已存在，已有发布日期 |
| 177 | US Online Gambling Market | ✅ 2025-07-17 | URI已存在，已有发布日期 |
| 178 | Online Lottery Market Size | ❌ 缺失 | URI已存在，日期提取失败 |
| 125 | The Growth of Lottery | ✅ 2024-05-31 | URI已存在，已有发布日期 |
| 22 | Lottery Market Size, Share, Demand | ✅ 2025-04-10 | URI已存在，已有发布日期 |
| 179 | Online Lottery Strategic Business Report | ✅ 2024-09-27 | URI已存在，已更新（刚刚修复） |
| 176 | O*NET OnLine | ❌ 缺失 | URI已存在，日期提取失败 |

### 4. 日期提取失败的原因

#### 文档175和178（grandviewresearch.com, expertmarketresearch.com）
- ❌ URL中无日期
- ❌ HTML meta标签中无日期信息
- ❌ JSON-LD中无日期信息
- ❌ HTTP响应头无Last-Modified
- ❌ 文本内容中无法提取到日期
- **结论**: 这些页面确实没有明确的发布日期信息

#### 文档179（finance.yahoo.com）
- ✅ 可以从文本内容提取: `2024-09-27`
- **已修复**: 刚刚通过手动测试更新成功

### 5. 总结

#### ✅ 去重逻辑是正确的
- URI去重是标准做法，避免重复插入相同URL的文档
- 轻量更新机制可以补充缺失的字段

#### ⚠️ 日期提取的局限性
- 部分页面确实没有明确的发布日期信息
- 对于这些页面，应该使用`created_at`作为备选方案

#### 💡 建议
1. **对于缺失日期的文档**:
   - 运行修复脚本: `python3 scripts/fix_publish_dates.py --fetch`
   - 如果仍然无法提取，使用`created_at`作为备选

2. **对于新摄取的文档**:
   - 增强的日期提取功能已集成
   - 会自动尝试多种方法提取日期
   - 如果都失败，会使用`created_at`作为备选

3. **监控去重效果**:
   - `skipped`数量高是正常的，说明去重工作正常
   - 关注`updated`数量，了解有多少文档被更新了

## 结论

**这些文档被跳过不是因为"内容重复"，而是因为"URI已存在"。**

这是**正确的去重行为**，避免了：
- 重复插入相同URL的文档
- 浪费存储空间
- 数据冗余

代码中的轻量更新机制可以：
- 补充缺失的字段（如publish_date）
- 更新文档类型、状态等信息
- 保持数据的完整性和准确性

