#!/usr/bin/env python3
"""
ä¸“é—¨æŸ¥æ‰¾æ•°æ®æºï¼šåˆ†æé¡µé¢é“¾æ¥ã€æŸ¥æ‰¾APIã€æŸ¥æ‰¾å†å²æ•°æ®é¡µé¢
"""

import httpx
from selectolax.parser import HTMLParser
import re
from urllib.parse import urljoin, urlparse
import json


def fetch_html(url: str):
    """è·å–HTML"""
    try:
        response = httpx.get(url, timeout=30, follow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        response.raise_for_status()
        return response.text, str(response.url)
    except Exception as e:
        return None, str(e)


def extract_all_links(html: str, base_url: str):
    """æå–æ‰€æœ‰é“¾æ¥å¹¶åˆ†ç±»"""
    parser = HTMLParser(html)
    all_links = parser.css("a[href]")
    
    categories = {
        "draw_games": [],
        "history": [],
        "results": [],
        "winning_numbers": [],
        "api": [],
        "other": [],
    }
    
    for link in all_links:
        href = link.attributes.get("href", "")
        text = link.text(strip=True).lower()
        href_lower = href.lower()
        
        try:
            full_url = urljoin(base_url, href)
        except:
            continue
        
        # åˆ†ç±»
        if any(kw in href_lower for kw in ["/draw-games/", "draw-game"]):
            categories["draw_games"].append({"text": link.text(strip=True), "href": href, "url": full_url})
        elif any(kw in href_lower or kw in text for kw in ["history", "past", "archive", "previous"]):
            categories["history"].append({"text": link.text(strip=True), "href": href, "url": full_url})
        elif any(kw in href_lower or kw in text for kw in ["result", "winning", "number"]):
            categories["results"].append({"text": link.text(strip=True), "href": href, "url": full_url})
        elif any(kw in href_lower for kw in ["api", "json", "/api/"]):
            categories["api"].append({"text": link.text(strip=True), "href": href, "url": full_url})
        else:
            categories["other"].append({"text": link.text(strip=True), "href": href, "url": full_url})
    
    return categories


def find_embedded_data(html: str):
    """æŸ¥æ‰¾åµŒå…¥çš„JSONæ•°æ®"""
    findings = []
    
    # æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„JSON
    script_pattern = r'<script[^>]*>([\s\S]*?)</script>'
    scripts = re.findall(script_pattern, html, re.IGNORECASE)
    
    for script in scripts:
        # æŸ¥æ‰¾JSONå¯¹è±¡
        json_patterns = [
            r'({[\s\S]{20,5000}?})',
            r'window\.__\w+\s*=\s*({.+?});',
            r'var\s+\w+\s*=\s*({.+?});',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, script, re.DOTALL)
            for match in matches[:3]:
                try:
                    data = json.loads(match)
                    if isinstance(data, dict) and len(data) > 2:
                        findings.append({
                            "type": "json_in_script",
                            "keys": list(data.keys())[:10],
                            "preview": str(data)[:200]
                        })
                        break
                except:
                    pass
    
    return findings


def analyze_winning_numbers_page():
    """åˆ†æwinning-numbersé¡µé¢"""
    print("\n" + "="*70)
    print("åˆ†æ: CA Lottery Winning Numbers é¡µé¢")
    print("="*70)
    
    url = "https://www.calottery.com/winning-numbers"
    html, final_url = fetch_html(url)
    
    if not html:
        print(f"âŒ æ— æ³•è®¿é—®: {final_url}")
        return
    
    parser = HTMLParser(html)
    print(f"âœ… æˆåŠŸè·å– ({len(html)} å­—ç¬¦)\n")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¸¸æˆé“¾æ¥
    game_links = parser.css("a[href*='draw-games']")
    print(f"ğŸ“‹ æ‰¾åˆ° {len(game_links)} ä¸ªæ¸¸æˆé“¾æ¥:")
    for link in game_links[:10]:
        href = link.attributes.get("href", "")
        text = link.text(strip=True)
        print(f"  - {text}: {href}")
    
    # æŸ¥æ‰¾æ˜¯å¦æœ‰å†å²æ•°æ®é“¾æ¥
    history_keywords = ["history", "past", "archive", "previous", "all"]
    history_links = []
    for link in parser.css("a[href]"):
        href = link.attributes.get("href", "").lower()
        text = link.text(strip=True).lower()
        if any(kw in href or kw in text for kw in history_keywords):
            history_links.append({
                "text": link.text(strip=True),
                "href": link.attributes.get("href", "")
            })
    
    if history_links:
        print(f"\nğŸ“š æ‰¾åˆ° {len(history_links)} ä¸ªå¯èƒ½çš„å†å²æ•°æ®é“¾æ¥:")
        for link in history_links[:10]:
            print(f"  - {link['text']}: {link['href']}")
    
    # æŸ¥æ‰¾è¡¨æ ¼
    tables = parser.css("table")
    print(f"\nğŸ“Š æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
    for i, table in enumerate(tables[:3]):
        rows = table.css("tr")
        print(f"  è¡¨æ ¼{i+1}: {len(rows)} è¡Œ")
        if rows:
            first_row = rows[0]
            cells = [cell.text(strip=True)[:30] for cell in first_row.css("td, th")]
            print(f"    åˆ—å¤´: {cells}")


def check_api_endpoints():
    """æ£€æŸ¥å¯èƒ½çš„APIç«¯ç‚¹"""
    print("\n" + "="*70)
    print("æ£€æŸ¥å¯èƒ½çš„APIç«¯ç‚¹")
    print("="*70)
    
    base_url = "https://www.calottery.com"
    
    # å¸¸è§çš„APIè·¯å¾„æ¨¡å¼
    api_paths = [
        "/api/v1/draws",
        "/api/draws",
        "/api/v1/winning-numbers",
        "/api/winning-numbers",
        "/api/v1/results",
        "/api/results",
        "/api/v1/games",
        "/data/draws.json",
        "/data/winning-numbers.json",
    ]
    
    found_apis = []
    for path in api_paths:
        url = base_url + path
        try:
            response = httpx.get(url, timeout=5, follow_redirects=True)
            if response.status_code == 200:
                content_type = response.headers.get("content-type", "")
                if "json" in content_type:
                    found_apis.append({
                        "url": url,
                        "status": response.status_code,
                        "content_type": content_type,
                        "size": len(response.text)
                    })
                    print(f"  âœ… {url} - {content_type} ({len(response.text)} å­—ç¬¦)")
                elif response.text[:100].strip().startswith("{"):
                    found_apis.append({
                        "url": url,
                        "status": response.status_code,
                        "content_type": content_type,
                        "size": len(response.text)
                    })
                    print(f"  âœ… {url} - å¯èƒ½æ˜¯JSON ({len(response.text)} å­—ç¬¦)")
        except Exception as e:
            pass
    
    if not found_apis:
        print("  âŒ æœªæ‰¾åˆ°å…¬å¼€çš„APIç«¯ç‚¹")
    
    return found_apis


def search_third_party_sources():
    """æœç´¢ç¬¬ä¸‰æ–¹æ•°æ®æº"""
    print("\n" + "="*70)
    print("æœç´¢ç¬¬ä¸‰æ–¹æ•°æ®æº")
    print("="*70)
    
    sources = [
        {
            "name": "LottoReport",
            "url": "https://www.lottoreport.com/california.htm",
            "description": "ç¬¬ä¸‰æ–¹å½©ç¥¨æ•°æ®ç½‘ç«™"
        },
        {
            "name": "Lottery Post",
            "url": "https://www.lotterypost.com/game/131",
            "description": "Powerballæ•°æ®"
        },
        {
            "name": "USAMega",
            "url": "https://www.usamega.com/mega-millions-history.asp",
            "description": "Mega Millionså†å²æ•°æ®"
        },
    ]
    
    results = []
    for source in sources:
        print(f"\nğŸ” æµ‹è¯•: {source['name']}")
        html, result = fetch_html(source["url"])
        if html:
            parser = HTMLParser(html)
            tables = parser.css("table")
            links = parser.css("a[href]")
            print(f"  âœ… å¯è®¿é—®")
            print(f"     - HTMLå¤§å°: {len(html)} å­—ç¬¦")
            print(f"     - è¡¨æ ¼æ•°é‡: {len(tables)}")
            print(f"     - é“¾æ¥æ•°é‡: {len(links)}")
            results.append({**source, "accessible": True, "tables": len(tables)})
        else:
            print(f"  âŒ æ— æ³•è®¿é—®: {result}")
            results.append({**source, "accessible": False})
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("æ•°æ®æºæŸ¥æ‰¾åˆ†æ")
    print("="*70)
    
    # 1. åˆ†æä¸»é¡µé¢é“¾æ¥
    print("\n" + "="*70)
    print("1. åˆ†æä¸»é¡µé¢é“¾æ¥ç»“æ„")
    print("="*70)
    
    main_url = "https://www.calottery.com/en/draw-games/superlotto-plus"
    html, final_url = fetch_html(main_url)
    
    if html:
        categories = extract_all_links(html, final_url)
        print(f"\nğŸ“‹ é“¾æ¥åˆ†ç±»:")
        for category, links in categories.items():
            if links:
                print(f"  {category}: {len(links)} ä¸ª")
                for link in links[:3]:
                    print(f"    - {link['text']}: {link['href']}")
        
        # æŸ¥æ‰¾åµŒå…¥æ•°æ®
        embedded = find_embedded_data(html)
        if embedded:
            print(f"\nğŸ“¦ æ‰¾åˆ° {len(embedded)} ä¸ªåµŒå…¥çš„JSONæ•°æ®")
            for data in embedded:
                print(f"  - keys: {data['keys']}")
    
    # 2. åˆ†æwinning-numbersé¡µé¢
    analyze_winning_numbers_page()
    
    # 3. æ£€æŸ¥APIç«¯ç‚¹
    apis = check_api_endpoints()
    
    # 4. æœç´¢ç¬¬ä¸‰æ–¹æ•°æ®æº
    third_party = search_third_party_sources()
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("æ€»ç»“")
    print("="*70)
    print(f"âœ… æ‰¾åˆ° {len(apis)} ä¸ªAPIç«¯ç‚¹")
    print(f"âœ… æµ‹è¯•äº† {len(third_party)} ä¸ªç¬¬ä¸‰æ–¹æ•°æ®æº")
    accessible_third = [s for s in third_party if s.get("accessible")]
    print(f"   - {len(accessible_third)} ä¸ªå¯è®¿é—®")


if __name__ == "__main__":
    main()

