#!/usr/bin/env python3
"""
æ£€æŸ¥é¡µé¢è¯¦ç»†ä¿¡æ¯ï¼ŒæŸ¥æ‰¾éšè—çš„æ•°æ®å’Œç»“æ„
"""

import httpx
from selectolax.parser import HTMLParser
import re
import json


def inspect_ca_page(url: str, name: str):
    """è¯¦ç»†æ£€æŸ¥CAé¡µé¢"""
    print("\n" + "="*70)
    print(f"è¯¦ç»†æ£€æŸ¥: {name}")
    print(f"URL: {url}")
    print("="*70)
    
    try:
        response = httpx.get(url, timeout=30, follow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        response.raise_for_status()
        html = response.text
        parser = HTMLParser(html)
        
        print(f"âœ… HTMLå¤§å°: {len(html)} å­—ç¬¦\n")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°æ®çš„section
        print("ğŸ“‘ æŸ¥æ‰¾é¡µé¢åŒºå—:")
        sections = parser.css("section, div[class*='section'], div[id*='section']")
        print(f"  æ‰¾åˆ° {len(sections)} ä¸ªsection/divåŒºå—")
        
        data_sections = []
        for section in sections[:10]:
            section_id = section.attributes.get("id", "")
            section_class = section.attributes.get("class", "")
            text_preview = section.text(strip=True)[:100]
            
            if any(kw in (section_id + " " + section_class).lower() for kw in ["draw", "result", "winning", "number", "history"]):
                data_sections.append({
                    "id": section_id,
                    "class": section_class,
                    "preview": text_preview
                })
        
        if data_sections:
            print(f"  æ‰¾åˆ° {len(data_sections)} ä¸ªæ•°æ®ç›¸å…³åŒºå—:")
            for sec in data_sections[:5]:
                print(f"    - id={sec['id']}, class={sec['class'][:50]}")
                print(f"      é¢„è§ˆ: {sec['preview']}")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼åŠå…¶ç»“æ„
        print("\nğŸ“Š è¯¦ç»†åˆ†æè¡¨æ ¼:")
        tables = parser.css("table")
        for i, table in enumerate(tables[:5]):
            print(f"\n  è¡¨æ ¼ {i+1}:")
            
            # æŸ¥æ‰¾thead
            thead = table.css_first("thead")
            if thead:
                headers = [th.text(strip=True) for th in thead.css("th, td")]
                print(f"    åˆ—å¤´: {headers}")
            
            # æŸ¥æ‰¾tbody
            tbody = table.css_first("tbody")
            if tbody:
                rows = tbody.css("tr")
                print(f"    è¡Œæ•°: {len(rows)}")
                
                # æ˜¾ç¤ºå‰3è¡Œæ•°æ®
                for j, row in enumerate(rows[:3]):
                    cells = [cell.text(strip=True) for cell in row.css("td, th")]
                    print(f"      è¡Œ{j+1}: {cells}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰dataå±æ€§
            data_attrs = {k: v for k, v in table.attributes.items() if k.startswith("data-")}
            if data_attrs:
                print(f"    dataå±æ€§: {data_attrs}")
        
        # 3. æŸ¥æ‰¾JavaScriptä¸­çš„æ•°æ®
        print("\nğŸ’» æŸ¥æ‰¾JavaScriptä¸­çš„æ•°æ®:")
        scripts = parser.css("script")
        found_data = False
        
        for script in scripts:
            script_text = script.text()
            if not script_text:
                continue
            
            # æŸ¥æ‰¾æ•°æ®å¯¹è±¡
            patterns = [
                (r'var\s+(\w+)\s*=\s*({[\s\S]{50,2000}?});', "varå¯¹è±¡"),
                (r'const\s+(\w+)\s*=\s*({[\s\S]{50,2000}?});', "constå¯¹è±¡"),
                (r'window\.(\w+)\s*=\s*({[\s\S]{50,2000}?});', "windowå¯¹è±¡"),
                (r'data:\s*({[\s\S]{50,2000}?})', "dataå¯¹è±¡"),
            ]
            
            for pattern, desc in patterns:
                matches = re.findall(pattern, script_text, re.DOTALL)
                for match in matches[:2]:
                    obj_str = match[1] if isinstance(match, tuple) else match
                    try:
                        obj = json.loads(obj_str)
                        if isinstance(obj, dict) and len(obj) > 2:
                            print(f"  âœ… æ‰¾åˆ°{desc}:")
                            print(f"     keys: {list(obj.keys())[:10]}")
                            if "draw" in str(obj).lower() or "result" in str(obj).lower():
                                print(f"     â­ å¯èƒ½åŒ…å«å¼€å¥–æ•°æ®!")
                            found_data = True
                    except:
                        pass
        
        if not found_data:
            print("  âŒ æœªæ‰¾åˆ°JavaScriptæ•°æ®å¯¹è±¡")
        
        # 4. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„AJAXè¯·æ±‚URL
        print("\nğŸŒ æŸ¥æ‰¾AJAXè¯·æ±‚URL:")
        ajax_patterns = [
            r'fetch\(["\']([^"\']+)["\']',
            r'\.get\(["\']([^"\']+)["\']',
            r'\.post\(["\']([^"\']+)["\']',
            r'url:\s*["\']([^"\']+)["\']',
            r'endpoint:\s*["\']([^"\']+)["\']',
            r'apiUrl:\s*["\']([^"\']+)["\']',
        ]
        
        ajax_urls = set()
        for script in scripts:
            script_text = script.text()
            for pattern in ajax_patterns:
                matches = re.findall(pattern, script_text, re.IGNORECASE)
                for match in matches:
                    if any(kw in match.lower() for kw in ["draw", "result", "winning", "number", "api", "data"]):
                        if match.startswith("/") or match.startswith("http"):
                            ajax_urls.add(match)
        
        if ajax_urls:
            print(f"  æ‰¾åˆ° {len(ajax_urls)} ä¸ªå¯èƒ½çš„AJAX URL:")
            for url_pattern in sorted(ajax_urls)[:10]:
                print(f"    - {url_pattern}")
        else:
            print("  âŒ æœªæ‰¾åˆ°AJAX URL")
        
        # 5. æŸ¥æ‰¾é¡µé¢ä¸Šæ˜¾ç¤ºçš„æ‰€æœ‰æ•°å­—ï¼ˆå¯èƒ½æ˜¯æ•°æ®ï¼‰
        print("\nğŸ”¢ åˆ†æé¡µé¢æ•°å­—æ¨¡å¼:")
        all_text = parser.body.text()
        
        # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
        dates = re.findall(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4}\b', all_text)
        unique_dates = sorted(set(dates))
        if unique_dates:
            print(f"  æ—¥æœŸ: {len(unique_dates)} ä¸ªå”¯ä¸€å€¼")
            print(f"    æœ€æ–°: {unique_dates[-1]}")
            print(f"    æœ€æ—©: {unique_dates[0] if len(unique_dates) > 1 else unique_dates[0]}")
        
        # æŸ¥æ‰¾é‡‘é¢æ¨¡å¼
        amounts = re.findall(r'\$[\d,]+(?:\.\d+)?\s*(?:million|billion|M|B|K)?', all_text, re.IGNORECASE)
        unique_amounts = sorted(set(amounts), key=lambda x: len(x), reverse=True)
        if unique_amounts:
            print(f"  é‡‘é¢: {len(unique_amounts)} ä¸ªå”¯ä¸€å€¼")
            print(f"    æœ€å¤§å€¼: {unique_amounts[0]}")
            print(f"    ç¤ºä¾‹: {unique_amounts[:5]}")
        
        # 6. æ£€æŸ¥æ˜¯å¦æœ‰"åŠ è½½æ›´å¤š"æˆ–"æŸ¥çœ‹å†å²"æŒ‰é’®
        print("\nğŸ”˜ æŸ¥æ‰¾äº¤äº’å…ƒç´ :")
        buttons = parser.css("button, a[class*='button'], a[class*='load'], a[class*='more'], a[class*='view']")
        relevant_buttons = []
        for btn in buttons:
            text = btn.text(strip=True).lower()
            onclick = btn.attributes.get("onclick", "").lower()
            href = btn.attributes.get("href", "").lower()
            
            if any(kw in (text + onclick + href) for kw in ["more", "load", "history", "past", "all", "view", "see"]):
                relevant_buttons.append({
                    "text": btn.text(strip=True),
                    "href": btn.attributes.get("href", ""),
                    "onclick": btn.attributes.get("onclick", "")[:100]
                })
        
        if relevant_buttons:
            print(f"  æ‰¾åˆ° {len(relevant_buttons)} ä¸ªç›¸å…³æŒ‰é’®:")
            for btn in relevant_buttons[:5]:
                print(f"    - {btn['text']}")
                if btn['href']:
                    print(f"      href: {btn['href']}")
                if btn['onclick']:
                    print(f"      onclick: {btn['onclick']}")
        else:
            print("  âŒ æœªæ‰¾åˆ°ç›¸å…³æŒ‰é’®")
        
        return {
            "tables": len(tables),
            "ajax_urls": list(ajax_urls),
            "dates_found": len(unique_dates),
            "amounts_found": len(unique_amounts),
        }
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def check_network_requests():
    """æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œæ£€æŸ¥ç½‘ç»œè¯·æ±‚"""
    print("\n" + "="*70)
    print("æ£€æŸ¥å¯èƒ½çš„ç½‘ç»œè¯·æ±‚æ¨¡å¼")
    print("="*70)
    
    # æ£€æŸ¥å¸¸è§çš„æ•°æ®è·å–æ¨¡å¼
    base_url = "https://www.calottery.com"
    
    # å¯èƒ½çš„APIè·¯å¾„ï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰
    possible_paths = [
        "/api/v1/lottery/draws",
        "/api/lottery/draws",
        "/api/v1/results",
        "/api/results",
        "/_api/draws",
        "/services/api/draws",
        "/en/api/draws",
        "/data/draws",
        "/winning-numbers/api",
    ]
    
    print("æµ‹è¯•å¯èƒ½çš„APIè·¯å¾„:")
    found = []
    for path in possible_paths:
        url = base_url + path
        try:
            response = httpx.get(url, timeout=3, follow_redirects=True)
            if response.status_code == 200:
                content_type = response.headers.get("content-type", "")
                if "json" in content_type or response.text.strip().startswith("{"):
                    found.append(url)
                    print(f"  âœ… {url}")
        except:
            pass
    
    if not found:
        print("  âŒ æœªæ‰¾åˆ°å¯ç”¨çš„APIè·¯å¾„")


def search_alternative_websites():
    """æœç´¢å…¶ä»–å¯èƒ½çš„ç½‘ç«™"""
    print("\n" + "="*70)
    print("æœç´¢å…¶ä»–æ•°æ®æºç½‘ç«™")
    print("="*70)
    
    websites = [
        {
            "name": "Powerball.com (å®˜æ–¹)",
            "url": "https://www.powerball.com",
            "note": "Powerballå…¨å›½å®˜ç½‘"
        },
        {
            "name": "MegaMillions.com (å®˜æ–¹)",
            "url": "https://www.megamillions.com",
            "note": "Mega Millionså…¨å›½å®˜ç½‘"
        },
        {
            "name": "Lottery USA",
            "url": "https://www.lotteryusa.com/california/",
            "note": "ç¬¬ä¸‰æ–¹æ•°æ®èšåˆ"
        },
    ]
    
    for site in websites:
        print(f"\nğŸ” {site['name']} ({site['note']})")
        try:
            response = httpx.get(site["url"], timeout=10, follow_redirects=True)
            if response.status_code == 200:
                parser = HTMLParser(response.text)
                tables = parser.css("table")
                links = parser.css("a[href*='result'], a[href*='draw'], a[href*='history']")
                print(f"  âœ… å¯è®¿é—®")
                print(f"     - HTML: {len(response.text)} å­—ç¬¦")
                print(f"     - è¡¨æ ¼: {len(tables)} ä¸ª")
                print(f"     - ç›¸å…³é“¾æ¥: {len(links)} ä¸ª")
                if links:
                    for link in links[:3]:
                        href = link.attributes.get("href", "")
                        print(f"       - {link.text(strip=True)[:30]}: {href[:60]}")
            else:
                print(f"  âš ï¸  çŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("æ·±åº¦ç½‘é¡µç»“æ„åˆ†æ - æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æº")
    print("="*70)
    
    # è¯¦ç»†æ£€æŸ¥CAé¡µé¢
    ca_pages = [
        ("CA SuperLotto Plus", "https://www.calottery.com/en/draw-games/superlotto-plus"),
        ("CA Powerball", "https://www.calottery.com/en/draw-games/powerball"),
        ("CA Winning Numbers", "https://www.calottery.com/winning-numbers"),
    ]
    
    results = []
    for name, url in ca_pages:
        result = inspect_ca_page(url, name)
        if result:
            results.append({**{"name": name, "url": url}, **result})
    
    # æ£€æŸ¥ç½‘ç»œè¯·æ±‚æ¨¡å¼
    check_network_requests()
    
    # æœç´¢å…¶ä»–ç½‘ç«™
    search_alternative_websites()
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("åˆ†ææ€»ç»“")
    print("="*70)
    for result in results:
        print(f"\n{result['name']}:")
        print(f"  - è¡¨æ ¼: {result.get('tables', 0)} ä¸ª")
        print(f"  - AJAX URL: {len(result.get('ajax_urls', []))} ä¸ª")
        print(f"  - æ—¥æœŸ: {result.get('dates_found', 0)} ä¸ª")
        print(f"  - é‡‘é¢: {result.get('amounts_found', 0)} ä¸ª")


if __name__ == "__main__":
    main()

