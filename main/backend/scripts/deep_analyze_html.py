#!/usr/bin/env python3
"""
æ·±åº¦åˆ†æç½‘é¡µç»“æ„ï¼ŒæŸ¥æ‰¾æ‰€æœ‰å¯æå–çš„ä¿¡æ¯å’Œå…¶ä»–æ•°æ®æº
"""

import httpx
from selectolax.parser import HTMLParser
import re
from urllib.parse import urljoin, urlparse
import json


def fetch_html_direct(url: str):
    """ç›´æ¥è·å–HTML"""
    try:
        response = httpx.get(url, timeout=30, follow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        response.raise_for_status()
        return response.text, response.url
    except Exception as e:
        raise RuntimeError(f"Failed to fetch {url}: {e}")


def analyze_page_structure(url: str, name: str):
    """æ·±åº¦åˆ†æé¡µé¢ç»“æ„"""
    print("\n" + "="*70)
    print(f"æ·±åº¦åˆ†æ: {name}")
    print(f"URL: {url}")
    print("="*70)
    
    try:
        html, final_url = fetch_html_direct(url)
        parser = HTMLParser(html)
        
        print(f"âœ… HTMLå¤§å°: {len(html)} å­—ç¬¦")
        print(f"   æœ€ç»ˆURL: {final_url}\n")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ•°æ®çš„å…ƒç´ 
        print("ğŸ” æŸ¥æ‰¾æ•°æ®ç›¸å…³å…ƒç´ :")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„å…ƒç´ 
        all_text = parser.body.text()
        dollar_amounts = re.findall(r'\$[\d,]+(?:\.\d+)?\s*(?:million|billion|M|B|K)?', all_text, re.IGNORECASE)
        if dollar_amounts:
            unique_amounts = sorted(set(dollar_amounts), key=lambda x: len(x), reverse=True)[:15]
            print(f"  ğŸ’° ç¾å…ƒé‡‘é¢: {len(unique_amounts)} ä¸ªå”¯ä¸€å€¼")
            print(f"     ç¤ºä¾‹: {unique_amounts[:5]}")
        
        # æŸ¥æ‰¾æ—¥æœŸ
        dates = re.findall(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4}\b', all_text)
        if dates:
            unique_dates = list(set(dates))[:10]
            print(f"  ğŸ“… æ—¥æœŸ: {len(unique_dates)} ä¸ªå”¯ä¸€å€¼")
            print(f"     ç¤ºä¾‹: {unique_dates[:3]}")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼ˆå¯»æ‰¾å†å²æ•°æ®ã€APIç­‰ï¼‰
        print("\nğŸ”— æŸ¥æ‰¾ç›¸å…³é“¾æ¥:")
        all_links = parser.css("a[href]")
        relevant_links = []
        
        keywords = [
            "history", "past", "archive", "results", "draw", 
            "previous", "winning", "numbers", "api", "json",
            "data", "export", "download", "report", "summary"
        ]
        
        for link in all_links:
            href = link.attributes.get("href", "")
            text = link.text(strip=True).lower()
            href_lower = href.lower()
            
            if any(keyword in href_lower or keyword in text for keyword in keywords):
                try:
                    full_url = urljoin(str(final_url), str(href))
                    relevant_links.append({
                        "text": link.text(strip=True)[:50],
                        "href": href,
                        "full_url": full_url
                    })
                except Exception:
                    pass
        
        if relevant_links:
            print(f"  æ‰¾åˆ° {len(relevant_links)} ä¸ªç›¸å…³é“¾æ¥:")
            for link in relevant_links[:10]:
                print(f"    - {link['text']}: {link['href']}")
                print(f"      â†’ {link['full_url']}")
        else:
            print("  âŒ æœªæ‰¾åˆ°ç›¸å…³é“¾æ¥")
        
        # 3. æŸ¥æ‰¾JSONæ•°æ®ï¼ˆå¯èƒ½åµŒå…¥åœ¨é¡µé¢ä¸­ï¼‰
        print("\nğŸ“¦ æŸ¥æ‰¾JSONæ•°æ®:")
        json_patterns = [
            r'<script[^>]*>[\s\S]*?({[\s\S]*?})[\s\S]*?</script>',
            r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
            r'window\.__DATA__\s*=\s*({.+?});',
            r'data:\s*({.+?})',
        ]
        
        found_json = False
        for pattern in json_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches[:3]:
                try:
                    data = json.loads(match)
                    if isinstance(data, dict) and len(data) > 0:
                        print(f"  âœ… æ‰¾åˆ°JSONæ•°æ® (keyæ•°é‡: {len(data)})")
                        print(f"     é¡¶å±‚keys: {list(data.keys())[:10]}")
                        found_json = True
                        break
                except:
                    pass
            if found_json:
                break
        
        if not found_json:
            print("  âŒ æœªæ‰¾åˆ°JSONæ•°æ®")
        
        # 4. æŸ¥æ‰¾å¯èƒ½çš„APIç«¯ç‚¹
        print("\nğŸŒ æŸ¥æ‰¾APIç«¯ç‚¹:")
        api_patterns = [
            r'["\']([^"\']*api[^"\']*)["\']',
            r'["\']([^"\']*json[^"\']*)["\']',
            r'fetch\(["\']([^"\']+)["\']',
            r'\.get\(["\']([^"\']+)["\']',
            r'url:\s*["\']([^"\']+)["\']',
        ]
        
        api_endpoints = set()
        for pattern in api_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if any(kw in match.lower() for kw in ["api", "json", "data", "draw", "result"]):
                    try:
                        full_url = urljoin(str(final_url), str(match))
                        if full_url.startswith("http"):
                            api_endpoints.add(full_url)
                    except Exception:
                        pass
        
        if api_endpoints:
            print(f"  æ‰¾åˆ° {len(api_endpoints)} ä¸ªå¯èƒ½çš„APIç«¯ç‚¹:")
            for endpoint in sorted(api_endpoints)[:10]:
                print(f"    - {endpoint}")
        else:
            print("  âŒ æœªæ‰¾åˆ°APIç«¯ç‚¹")
        
        # 5. æŸ¥æ‰¾è¡¨æ ¼ç»“æ„
        print("\nğŸ“Š æŸ¥æ‰¾è¡¨æ ¼:")
        tables = parser.css("table")
        print(f"  æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
        for i, table in enumerate(tables[:5]):
            thead = table.css_first("thead")
            tbody = table.css_first("tbody")
            if thead:
                headers = [th.text(strip=True) for th in thead.css("th, td")]
                print(f"  è¡¨æ ¼{i+1}åˆ—å¤´: {headers}")
            if tbody:
                rows = tbody.css("tr")
                print(f"    è¡Œæ•°: {len(rows)}")
                if rows:
                    first_row = rows[0]
                    cells = [cell.text(strip=True)[:30] for cell in first_row.css("td, th")]
                    print(f"    ç¬¬ä¸€è¡Œ: {cells}")
        
        # 6. æŸ¥æ‰¾metaæ ‡ç­¾å’Œdataå±æ€§
        print("\nğŸ·ï¸  æŸ¥æ‰¾metaæ ‡ç­¾å’Œdataå±æ€§:")
        meta_tags = parser.css("meta[property], meta[name]")
        relevant_meta = []
        for meta in meta_tags[:10]:
            prop = meta.attributes.get("property") or meta.attributes.get("name")
            content = meta.attributes.get("content", "")
            if any(kw in content.lower() for kw in ["draw", "jackpot", "winner", "number"]):
                relevant_meta.append(f"{prop}: {content[:50]}")
        
        if relevant_meta:
            print(f"  æ‰¾åˆ°ç›¸å…³metaæ ‡ç­¾:")
            for meta in relevant_meta:
                print(f"    - {meta}")
        
        data_attrs = parser.css("[data-*]")
        if data_attrs:
            data_keys = set()
            for elem in data_attrs[:20]:
                for attr in elem.attributes:
                    if attr.startswith("data-"):
                        data_keys.add(attr)
            if data_keys:
                print(f"  æ‰¾åˆ°dataå±æ€§: {sorted(data_keys)[:10]}")
        
        # 7. æŸ¥æ‰¾å¯èƒ½çš„URLæ¨¡å¼ï¼ˆå†å²æ•°æ®ï¼‰
        print("\nğŸ” æŸ¥æ‰¾URLæ¨¡å¼:")
        url_patterns = set()
        for link in all_links:
            href = link.attributes.get("href", "")
            if any(kw in href.lower() for kw in ["draw", "result", "history", "past"]):
                # æå–URLæ¨¡å¼
                parts = href.split("/")
                if len(parts) > 2:
                    pattern = "/".join(parts[:3]) + "/..."
                    url_patterns.add(pattern)
        
        if url_patterns:
            print(f"  å¯èƒ½çš„URLæ¨¡å¼:")
            for pattern in sorted(url_patterns)[:10]:
                print(f"    - {pattern}")
        
        return {
            "url": final_url,
            "relevant_links": relevant_links,
            "api_endpoints": list(api_endpoints),
            "tables_count": len(tables),
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def search_alternative_sources():
    """æœç´¢å…¶ä»–æ•°æ®æº"""
    print("\n" + "="*70)
    print("æœç´¢å…¶ä»–æ•°æ®æº")
    print("="*70)
    
    sources = [
        {
            "name": "CA Lottery Winning Numbers",
            "url": "https://www.calottery.com/winning-numbers",
        },
        {
            "name": "CA Lottery All Games",
            "url": "https://www.calottery.com/en/draw-games",
        },
        {
            "name": "CA Lottery News/Releases",
            "url": "https://www.calottery.com/news-releases",
        },
    ]
    
    results = []
    for source in sources:
        print(f"\nğŸ” æµ‹è¯•: {source['name']}")
        try:
            result = analyze_page_structure(source["url"], source["name"])
            if result:
                results.append({**source, **result})
        except Exception as e:
            print(f"  âš ï¸  è·³è¿‡: {e}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("æ·±åº¦ç½‘é¡µç»“æ„åˆ†æ - æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æº")
    print("="*70)
    
    # åˆ†æä¸»é¡µé¢
    main_pages = [
        ("CA SuperLotto Plus", "https://www.calottery.com/en/draw-games/superlotto-plus"),
        ("CA Powerball", "https://www.calottery.com/en/draw-games/powerball"),
        ("CA Mega Millions", "https://www.calottery.com/en/draw-games/mega-millions"),
    ]
    
    main_results = []
    for name, url in main_pages:
        result = analyze_page_structure(url, name)
        if result:
            main_results.append({**{"name": name, "url": url}, **result})
    
    # æœç´¢å…¶ä»–æ•°æ®æº
    alt_results = search_alternative_sources()
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("åˆ†ææ€»ç»“")
    print("="*70)
    
    print("\nğŸ“‹ ä¸»é¡µé¢å‘ç°:")
    for result in main_results:
        print(f"  {result['name']}:")
        print(f"    - ç›¸å…³é“¾æ¥: {len(result.get('relevant_links', []))} ä¸ª")
        print(f"    - APIç«¯ç‚¹: {len(result.get('api_endpoints', []))} ä¸ª")
        print(f"    - è¡¨æ ¼: {result.get('tables_count', 0)} ä¸ª")
    
    print("\nğŸ“‹ å…¶ä»–æ•°æ®æºå‘ç°:")
    for result in alt_results:
        print(f"  {result['name']}:")
        print(f"    - URL: {result['url']}")
        print(f"    - ç›¸å…³é“¾æ¥: {len(result.get('relevant_links', []))} ä¸ª")
        print(f"    - APIç«¯ç‚¹: {len(result.get('api_endpoints', []))} ä¸ª")
        print(f"    - è¡¨æ ¼: {result.get('tables_count', 0)} ä¸ª")


if __name__ == "__main__":
    main()

