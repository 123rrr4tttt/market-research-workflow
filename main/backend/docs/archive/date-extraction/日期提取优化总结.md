# 日期提取优化总结报告

## 执行时间
2025-11-07

## 一、优化历程

### 1.1 初始状态
- **缺失发布日期的文档**: 81个
- **可提取率**: 8.6% (7/81)
- **提取来源**: 仅从content文本内容

### 1.2 第一次优化
- **改进内容**: 
  - 增强文本内容提取（支持美式日期、英文月份格式）
  - 改进HTML解析（检查time标签、CSS类）
  - 完善extracted_data检查
- **结果**: 8/81 (9.9%)

### 1.3 第二次优化（链接分析后）
- **改进内容**:
  - 支持time标签时区格式
  - 递归检查JSON-LD
  - 支持更多日期格式
- **结果**: 8/81 (9.9%) - 逻辑更健壮

### 1.4 最终优化（重新抓取）
- **改进内容**:
  - 优先提取日期部分（YYYY-MM-DD）
  - 支持微秒格式（`.000Z`）
  - 支持时区格式（`+00:00`）
  - **添加重新抓取功能**（可选，默认禁用）
  - 改进JSON-LD递归检查
  - 添加进度显示和超时处理
- **结果**: 
  - 样本测试: 9/20 (45%)
  - **最终修复: 20个文档**（从81个减少到61个）
  - **总体覆盖率: 59%** (88/149)

## 二、最终优化成果

### 2.1 支持的日期格式

#### Meta标签格式
- `2025-04-07` - 标准日期
- `2025-04-07T13:30:19` - ISO日期时间
- `2025-04-07T13:30:19Z` - UTC时间
- `2025-07-29T21:37:09.000Z` - 带微秒的UTC时间
- `2025-04-07T13:30:19+00:00` - 带时区的时间

#### Time标签格式
- `2025-01-22T22:38:37-0800` - 美西时区
- `2025-01-22T22:38:37+0800` - 东八区
- `2025-04-07 12:00` - 简化格式

#### JSON-LD格式
- 递归检查嵌套对象
- 支持多种字段名：`datePublished`, `publishedTime`, `dateCreated`, `dateModified`等
- 支持列表中的日期字符串

#### 文本格式
- ISO格式: `2024-01-15`
- 美式格式: `01/15/2024` 或 `1/15/2024`
- 英文格式: `January 15, 2024` 或 `Jan 15, 2024`

### 2.2 提取策略

**优先级顺序**:
1. 从URL中提取（如果URL包含日期）
2. 从保存的HTML content中提取
3. **重新抓取页面并提取**（新增）
4. 从extracted_data中提取

**提取方法优先级**:
1. Meta标签（最可靠）
2. Time标签
3. JSON-LD结构化数据
4. 常见日期CSS类/ID
5. 文本内容正则匹配

### 2.3 关键改进点

#### 1. 优先提取日期部分
```python
# 直接提取前10个字符 YYYY-MM-DD
dt = datetime.strptime(date_str[:10], '%Y-%m-%d')
```
这确保了即使时间戳格式复杂，也能可靠提取日期。

#### 2. 重新抓取功能
```python
# 如果content中没有找到，尝试重新抓取
if not new_date and doc.uri:
    html_content, _ = fetch_html(doc.uri)
    new_date = extract_date_from_html_content(html_content, doc.uri)
```
成功提取到文档8、20、22的日期。

#### 3. 递归JSON-LD检查
- 防止循环引用
- 检查嵌套对象
- 检查列表中的日期字符串

## 三、测试结果

### 3.1 样本测试（20个文档）
- **检查**: 20个
- **修复**: 9个
- **跳过**: 11个
- **提取率**: 45%

### 3.2 成功提取的文档示例

| 文档ID | 标题 | 来源 | 提取方法 | 日期 |
|--------|------|------|----------|------|
| 7 | Lone Star Crackdown... | regulatoryoversight.com | Meta标签 | 2025-04-07 |
| 8 | California Lottery strikes... | kmph.com | 重新抓取+Meta标签 | 2025-07-29 |
| 20 | Lottery Market Report... | lucintel.com | 重新抓取+JSON-LD | 2025-06-13 |
| 22 | Lottery Market Size... | market.us | 重新抓取+JSON-LD | 2025-04-10 |

### 3.3 仍无法提取的文档

**主要原因**:
1. **PDF文件** (约10个)
   - 需要PDF元数据解析
   - 或从文件名/URL中提取

2. **静态页面** (约25个)
   - calottery.com的FAQ、首页等
   - 没有明确的发布日期
   - 建议使用 `created_at` 作为备选

3. **Reddit帖子** (11个)
   - 需要从Reddit API获取时间戳
   - 或解析URL获取帖子ID

4. **其他** (约26个)
   - 数据页面、搜索结果页面等
   - 没有明确的发布日期

## 四、性能考虑

### 4.1 重新抓取的权衡

**优点**:
- 能获取最新的HTML内容
- 成功提取到3个额外文档

**缺点**:
- 需要网络请求，速度较慢
- 可能被网站限制或封禁
- 增加服务器负载

**建议**:
- 只对无法从content提取的文档重新抓取
- 可以添加缓存机制
- 可以添加重试和错误处理

### 4.2 优化建议

1. **批量处理**
   - 可以分批处理，避免一次性处理太多
   - 添加进度显示

2. **缓存机制**
   - 缓存重新抓取的HTML内容
   - 避免重复抓取

3. **错误处理**
   - 添加重试机制
   - 记录失败原因

## 五、下一步建议

### 5.1 立即执行
1. ✅ 运行完整修复脚本
2. 对无法提取的文档使用 `created_at` 作为备选

### 5.2 短期改进（1周内）
1. **PDF文件处理**
   - 解析PDF元数据
   - 或从文件名中提取日期

2. **Reddit来源**
   - 从Reddit API获取时间戳
   - 或解析URL获取帖子ID

3. **改进数据摄取流程**
   - 在摄取时就提取发布日期
   - 保存完整的HTML内容

### 5.3 长期改进（1个月内）
1. **建立规则库**
   - 为每个主要来源建立提取规则
   - 定期更新和维护

2. **监控和告警**
   - 监控新摄取文档的提取率
   - 如果提取率下降，及时告警

3. **使用LLM辅助**
   - 对于复杂格式，使用LLM识别发布日期
   - 提高准确率

## 六、总结

### 6.1 优化成果

✅ **提取能力大幅提升**:
- 从8.6%提升到45%（样本测试）
- 支持更多日期格式
- 添加重新抓取功能

✅ **代码质量提升**:
- 更健壮的日期解析
- 更好的错误处理
- 递归检查JSON-LD

### 6.2 最终结果

✅ **实际修复成果**:
- **第一轮修复**: 20个文档（从81个减少到61个）
- **Reddit专项修复**: 10个文档（使用Reddit JSON API）
- **总计修复**: 30个文档
- **修复前**: 81个文档缺少发布日期
- **修复后**: 51个文档缺少发布日期
- **总体覆盖率**: 66% (98/149)

✅ **提取来源统计**:
- 从URL提取: 1个
- 从HTML content提取: 约15个
- 通过重新抓取提取: 约4个
- **从Reddit JSON API提取: 10个** ✨

⚠️ **剩余51个文档**:
- PDF文件: 约10个（需要特殊处理）
- 静态页面: 约25个（建议使用created_at）
- Reddit帖子: 0个（✅ 已全部修复）
- 其他: 约16个

### 6.3 Reddit专项优化

✅ **Reddit日期提取实现**:
- 使用Reddit JSON API（无需API凭证）
- URL格式: `https://www.reddit.com/r/subreddit/comments/POST_ID/title/.json`
- 从返回的JSON中提取 `created_utc` 时间戳
- **成功修复所有Reddit文档**（36个全部修复！）

**Reddit修复统计**:
- Reddit文档总数: 36个
- 已修复: 36个 (100%)
- 本次修复: 10个（之前已有26个）

**实现方法**:
```python
# 在Reddit URL后加.json访问JSON API
json_url = url.rstrip('/') + '.json'
response = requests.get(json_url)
data = response.json()
created_utc = data[0]['data']['children'][0]['data']['created_utc']
date = datetime.fromtimestamp(created_utc).date()
```

### 6.4 后续建议

- **短期**: 对剩余51个文档使用 `created_at` 作为备选
- **中期**: 针对PDF文件定制提取逻辑（解析PDF元数据或文件名）
- **长期**: 改进数据摄取流程，确保新文档能自动提取

### 6.5 技术亮点

1. **Reddit JSON API集成**: 使用标准方法，无需API凭证，100%成功率
2. **智能提取策略**: 多层级提取，从最可靠到最灵活
3. **稳定性优化**: 超时处理、进度显示、错误恢复
4. **可扩展性**: 易于添加新的提取方法和来源

### 6.6 关键改进

1. **优先提取日期部分** - 提高了可靠性
2. **重新抓取功能** - 增加了提取机会（可选，避免卡住）
3. **递归JSON-LD检查** - 提高了覆盖率
4. **支持更多格式** - 提高了兼容性
5. **进度显示和超时处理** - 提高了稳定性
6. **Reddit JSON API** - 标准方法，无需凭证

## 七、执行总结

### 7.1 修复执行

**执行命令**:
```bash
python scripts/fix_publish_dates.py
```

**执行结果**:
- ✅ 成功修复20个文档
- ✅ 脚本运行稳定，无卡死问题
- ✅ 添加了进度显示

### 7.2 修复的文档示例

成功修复的文档包括：
- 政府网站（ca.gov, oag.ca.gov）
- 新闻网站（cbsnews.com, foxla.com, kmph.com）
- 市场报告网站（lucintel.com, market.us, fool.com）
- 彩票官网（calottery.com, powerball.com）
- 其他来源（Reddit, Google Play等）

### 7.3 脚本使用说明

**基本用法**:
```bash
# 预览模式（不实际更新）
python scripts/fix_publish_dates.py --dry-run

# 实际修复（不重新抓取，快速）
python scripts/fix_publish_dates.py

# 启用重新抓取（较慢，但能提取更多）
python scripts/fix_publish_dates.py --fetch

# 限制处理数量（测试用）
python scripts/fix_publish_dates.py --limit=10
```

**注意事项**:
- 默认不启用重新抓取，避免卡住
- 重新抓取功能需要网络连接，可能较慢
- PDF文件会自动跳过重新抓取

